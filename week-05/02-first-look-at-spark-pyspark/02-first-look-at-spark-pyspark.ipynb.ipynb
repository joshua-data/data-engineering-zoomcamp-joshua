{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download \"High Volume For-Hire Vehicle Trip Records\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! wget https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since it's PARQUET, I need to convert it to CSV. (, which has not been dealt with in the course video.)\n",
    "    * It will take some minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fname = 'fhvhv_tripdata_2024-01'\n",
    "fpath_parquet = fname + '.parquet'\n",
    "fpath_csv = fname + '.csv'\n",
    "\n",
    "df = pd.read_parquet(fpath_parquet)\n",
    "df.to_csv(fpath_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! wc -l {fpath_csv}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the File with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Go to Spark UI, then you'll notice a job is created named `csv at NativeMethodAccessorImpl.java:0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .csv(fpath_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All the Columns are typed in String by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run each.\n",
    "df.schema\n",
    "# df.show(5)\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a small sample file from the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! head -n 101 {fpath_csv} > head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! head -n 5 head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! wc -l head.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fpath_head_csv = 'head.csv'\n",
    "df_pandas = pd.read_csv(fpath_head_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By the way, Pandas infers each column to integer and float. (Unlike the Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_pandas.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enforce a Custom Schema using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a custom schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "schema = types.StructType(\n",
    "    [\n",
    "        types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "        types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "        types.StructField('originating_base_num', types.StringType(), True),\n",
    "        types.StructField('request_datetime', types.TimestampType(), True),\n",
    "        types.StructField('on_scene_datetime', types.TimestampType(), True),\n",
    "        types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "        types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "        types.StructField('PULocationID', types.IntegerType(), True),\n",
    "        types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "        types.StructField('trip_miles', types.FloatType(), True),\n",
    "        types.StructField('trip_time', types.IntegerType(), True),\n",
    "        types.StructField('base_passenger_fare', types.FloatType(), True),\n",
    "        types.StructField('tolls', types.FloatType(), True),\n",
    "        types.StructField('bcf', types.FloatType(), True),\n",
    "        types.StructField('sales_tax', types.FloatType(), True),\n",
    "        types.StructField('congestion_surcharge', types.FloatType(), True),\n",
    "        types.StructField('airport_fee', types.FloatType(), True),\n",
    "        types.StructField('tips', types.FloatType(), True),\n",
    "        types.StructField('driver_pay', types.FloatType(), True),\n",
    "        types.StructField('shared_request_flag', types.StringType(), True),\n",
    "        types.StructField('shared_match_flag', types.StringType(), True),\n",
    "        types.StructField('access_a_ride_flag', types.StringType(), True),\n",
    "        types.StructField('wav_request_flag', types.StringType(), True),\n",
    "        types.StructField('wav_match_flag', types.StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .schema(schema) \\\n",
    "    .csv(fpath_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, you'll find each column has more proper data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as a Parquet File (using Partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Repartition doesn't trigger repartitioning yet, it'll be repartitioned when you try to save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.repartition(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save data as a parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.write.parquet('fhvhv/2024/01/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Open a terminal and go to `fhvhv/2024/01/`, then you'll find 24 partitions have been created."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
